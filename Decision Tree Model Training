import numpy as np
import matplotlib.pyplot as plt
import polars as pl
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import ValidationCurveDisplay
from sklearn.svm import SVC
from joblib import dump, load
from Projet_IA import data_0
# explicitly require this experimental feature
from sklearn.experimental import enable_halving_search_cv  # noqa
# now you can import normally from model_selection
from sklearn.model_selection import HalvingGridSearchCV
from sklearn.model_selection import HalvingRandomSearchCV

#data=pl.read_csv("Features_by_window_size/sero_features_4.csv")
data=data_0.filtered
print (data)

X = data[:, 1:len(data)] #les caractéristiques
y = data[:, 0]  #les résulats (classes)

X = X.to_numpy()
y = y.to_numpy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)

modele_clf = DecisionTreeClassifier()
modele_clf.fit(X_train, y_train)

y_pred = modele_clf.predict(X_test)
y_pred2 = modele_clf.predict(X_train)
print("CLF precsion en test: ", accuracy_score(y_test, y_pred))
print("CLF precsion en entrainement: ", accuracy_score(y_train, y_pred2))

report = classification_report(y_test, y_pred)
print(print("CLF Classification Report on test:\n", report))
# Le classification report donne la précision du modèle par classe

# Perform cross-validation
cv_scores = cross_val_score(modele_clf, X_test, y_test, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Score:", cv_scores.mean())
# La validation croisée teste le modèle 5 fois de suite dans l'ensemble de test.

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 15 ,20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4, 6], 
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_leaf_nodes': [None, 5, 10, 15,  20, 25, 30],
    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3],
}
HGSearch = HalvingGridSearchCV(modele_clf, param_grid, cv=5, factor=2, max_resources=100)
HGSearch.fit(X, y)

print("Best parameters found: ", HGSearch.best_params_)
print("Best score: ", HGSearch.best_score_)

df = pl.DataFrame(HGSearch.cv_results_)

# ValidationCurveDisplay.from_estimator(
#    SVC(kernel="linear"), X, y, param_name="C", param_range=np.logspace(-7, 3, 10)
# )
plt.show()

# dump( modele_clf, 'Vehicle_prediction_DecisionTree.joblib')